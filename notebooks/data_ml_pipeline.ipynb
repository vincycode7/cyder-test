{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import spacy, os\n",
    "import hashlib, requests, re, spacy\n",
    "from typing import List, Optional, Tuple\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_download_models():\n",
    "    spacy_model_name = 'en_core_web_sm'\n",
    "    try:\n",
    "        nlp = spacy.load(spacy_model_name)\n",
    "    except OSError:\n",
    "        spacy.cli.download(spacy_model_name)\n",
    "        try:\n",
    "            nlp = spacy.load(spacy_model_name)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed Process Due to {e}\")\n",
    "    output_dir=\"outputs/data/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  \n",
    "    print(f\"os.path.exists(output_dir): {os.path.exists(output_dir)}\")\n",
    "load_or_download_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analysis, Data Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalyzer:\n",
    "    def __init__(self, link_to_csv: str, pii_to_mask=None, pii_to_remove=None, output_file=None, output_dir=\"outputs/data/\"):\n",
    "        \"\"\"\n",
    "        A class for analyzing data.\n",
    "\n",
    "        Parameters:\n",
    "        data (List[dict]): The data to be analyzed.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(link_to_csv)\n",
    "        print(f\"output_dir: {output_dir}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)   \n",
    "                     \n",
    "        self.output_file = output_dir+\"output_analysis_\"+link_to_csv.split(\"/\")[-1]\n",
    "        self.pii_to_mask = pii_to_mask or ['userId']\n",
    "        self.pii_to_remove = pii_to_remove or ['userId','ip']\n",
    "        self.metadata_data = ['metadata.name', 'metadata.content']\n",
    "        spacy_model_name = 'en_core_web_sm'\n",
    "        self.nlp = spacy.load(spacy_model_name)\n",
    "        \n",
    "    def gentle_preprocess(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        A gentle data preprocessing method that replaces NaN values with empty strings and strips whitespace from the remaining values.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input data to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The preprocessed data.\n",
    "        \"\"\"\n",
    "        return data.fillna(\"\").apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "    def drop_pii_col(self, cols: Optional[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Removes personally identifiable information (PII) from the data.\n",
    "\n",
    "        Parameters:\n",
    "        cols (Optional[List[str]]): The list of column names to remove. If not provided,\n",
    "        the default list of PII columns ['userId', 'ip'] will be used.\n",
    "        \"\"\"\n",
    "        cols = cols or self.pii_to_remove or ['userId']\n",
    "        missing_cols = set(cols) - set(self.data.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Columns {missing_cols} not found in DataFrame\")\n",
    "\n",
    "        self.data = self.data.drop(columns=[col for col in cols if col in self.data.columns], errors='ignore')\n",
    "    \n",
    "    def mask_pii(self, cols: Optional[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Hashes the specified columns in the data using the SHA256 algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        cols (Optional[List[str]]): The list of column names to hash. Defaults to ['userId'].\n",
    "        \"\"\"\n",
    "        cols = cols or self.pii_to_mask or ['userId']\n",
    "        missing_cols = set(cols) - set(self.data.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Columns {missing_cols} not found in DataFrame\")\n",
    "\n",
    "        for col in cols:\n",
    "            self.data[col] = self.data[col].apply(lambda x: hashlib.sha256(str(x).encode('utf-8')).hexdigest())\n",
    "\n",
    "    def remove_pii_from_metadata(self, cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Removes personally identifiable information (PII) from the metadata of a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            cols (Optional[List[str]]): A list of column names to remove PII from. Default is ['userId'].\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The modified DataFrame with PII removed.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any column names in `cols` are not present in the DataFrame.\n",
    "        \"\"\"\n",
    "        cols = cols or self.metadata_data or ['metadata.name', 'metadata.content']\n",
    "        missing_cols = set(cols) - set(self.data.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Columns {missing_cols} not found in DataFrame\")\n",
    "\n",
    "        for col in cols:\n",
    "            if col in ['metadata.name', 'metadata.content']:\n",
    "              self.data[col] = self.data[col].apply(lambda x: re.sub(r\"\\b\\d{9}\\b|\\b\\d{3}[-.]?\\d{2}[-.]?\\d{4}\\b\", \"\", str(x)))\n",
    "        return self.data\n",
    "\n",
    "    def anonymize_user(self):\n",
    "        self.mask_pii()\n",
    "        self.drop_pii_col()\n",
    "        self.remove_pii_from_metadata()\n",
    "\n",
    "    def extract_interests_and_demo(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts interests from metadata.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The modified DataFrame with interests extracted.\n",
    "        \"\"\"\n",
    "        if 'metadata.content' not in self.data.columns:\n",
    "            raise ValueError(\"Column 'metadata.content' not found in DataFrame\")\n",
    "            \n",
    "        interests, income_range, is_finance_related = zip(*self.data['metadata.content'].apply(self.extract_interests_and_demo_from_text))\n",
    "        self.data['interests'] = interests\n",
    "        # self.data['age_group'] = age_group\n",
    "        self.data['income_range'] = income_range\n",
    "        self.data['is_finance_related'] = is_finance_related\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def extract_interests_and_demo_from_text(self, text: str):\n",
    "        doc = self.nlp(text)\n",
    "        interests = self.extract_interests_from_doc(doc)\n",
    "        # age_group = self.extract_age_group_from_doc(doc)\n",
    "        income_range = self.extract_income_range_from_doc(doc)\n",
    "        is_finance_related = self.extract_is_finance_related_from_doc(text)\n",
    "        return interests, income_range, is_finance_related\n",
    "\n",
    "    def extract_interests_from_doc(self, doc: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts interests from a given text using spaCy.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to extract interests from.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of extracted interests with their associated named entities.\n",
    "        \"\"\"\n",
    "        interests = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PRODUCT\", \"ORG\", \"GPE\"]:\n",
    "                interests.append(ent.text)\n",
    "        return ' '.join(interests)\n",
    "\n",
    "    def extract_age_group_from_doc(self, doc: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Extracts age group from a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to extract age group from.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted age group.\n",
    "        \"\"\"\n",
    "        age_group = \"\"\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"AGE\":\n",
    "                age_group = ent.text\n",
    "        return age_group\n",
    "\n",
    "    def extract_income_range_from_doc(self, doc: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Extracts income range from a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to extract income range from.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted income range.\n",
    "        \"\"\"\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"MONEY\":\n",
    "                # clean the money value\n",
    "                income_str = ent.text.replace(\",\", \"\").replace(\"$\", \"\")\n",
    "                # convert the cleaned value to float\n",
    "                try:\n",
    "                    income = float(income_str)\n",
    "                except ValueError:\n",
    "                    return \"undefined\"\n",
    "                # map the income to a text range\n",
    "                return self.map_income_to_range(income)\n",
    "        # no money entity found\n",
    "        return \"undefined\"\n",
    "\n",
    "    def map_income_to_range(self,income: float) -> str:\n",
    "        \"\"\"\n",
    "        Maps income value to a text income range.\n",
    "\n",
    "        Args:\n",
    "            income (float): The income value to map.\n",
    "\n",
    "        Returns:\n",
    "            str: The mapped income range.\n",
    "        \"\"\"\n",
    "        if income < 0:\n",
    "            return \"Undefined\"\n",
    "        elif income < 100000:\n",
    "            return \"Below $100,000\"\n",
    "        else:\n",
    "            return \"Above $100,000\"\n",
    "\n",
    "    def extract_is_finance_related_from_doc(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if the given text is finance related.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the text is finance related, False otherwise.\n",
    "        \"\"\"\n",
    "        finance_keywords = ['finance', 'financial', 'invest', 'investment', 'stock',     \n",
    "                            'portfolio', 'mutual fund', 'bond', 'wealth management',     \n",
    "                            'hedge fund', 'equity', 'market', 'capital', 'fund',     \n",
    "                            'asset', 'commodity', 'derivatives', 'trading', 'brokerage',     \n",
    "                            'valuation', 'risk management', 'options', 'futures',     \n",
    "                            'real estate investment trust', 'private equity',     \n",
    "                            'venture capital', 'insurance', 'retirement planning',     \n",
    "                            'taxation', 'credit', 'loan', 'banking', 'accounting',     \n",
    "                            'audit', 'financial planning', 'economic', 'macroeconomics',     \n",
    "                            'microeconomics', 'monetary policy', 'fiscal policy',     \n",
    "                            'budget', 'debt', 'interest rate', 'inflation',     \n",
    "                            'exchange rate', 'foreign exchange', 'cryptocurrency',     \n",
    "                            'blockchain', 'digital currency', 'initial coin offering',     \n",
    "                            'smart contract', 'decentralized finance', 'financial technology',     \n",
    "                            'payment system', 'credit card', 'debit card',     \n",
    "                            'mobile payment', 'e-commerce', 'online payment',     \n",
    "                            'crowdfunding', 'peer-to-peer lending', 'robo-advisor',     \n",
    "                            'artificial intelligence in finance',     \n",
    "                            'machine learning in finance',     \n",
    "                            'quantitative finance'\n",
    "                            ]\n",
    "\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        if set(finance_keywords) & set(tokens):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def derive_attributes(self) -> None:\n",
    "        \"\"\"\n",
    "        Derives demographic and interest-based attributes from the data.\n",
    "        \"\"\"\n",
    "        self.extract_interests_and_demo()\n",
    "\n",
    "    def save_data(self, output_file: str=None) -> None:\n",
    "        \"\"\"\n",
    "        Saves the analyzed data to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            output_file (str): The file path of the output CSV file.\n",
    "        \"\"\"\n",
    "        output_file = output_file or self.output_file\n",
    "        print(f\"save file with : {output_file}\")\n",
    "        self.data.to_csv(output_file, index=False)\n",
    "\n",
    "    def analyze_data(self):\n",
    "        \"\"\"\n",
    "        Analyzes the data and returns a summary of insights.\n",
    "        \"\"\"\n",
    "        self.data = self.gentle_preprocess(self.data)\n",
    "        self.anonymize_user()\n",
    "        self.derive_attributes()\n",
    "        self.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataAnalyzer(link_to_csv='inputs/UserWillChristodoulou_DataScience_TestData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data before engineering\n",
    "data_processor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.analyze_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data after engineering\n",
    "data_processor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking demography data\n",
    "\n",
    "data_processor.data[data_processor.data.income_range != \"undefined\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineered feature to get if user is interested in finance product\n",
    "data_processor.data[data_processor.data.is_finance_related == True].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model Template (Machine Learning) Note: This Pipeline is not implemented as current data is not sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, input_file: str=None):\n",
    "        self.input_file = input_file\n",
    "\n",
    "    def extract_ml_features_target(self) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        # Extract features and target from the processed data\n",
    "        pass\n",
    "\n",
    "\n",
    "class DataSplitter:\n",
    "    def __init__(self, X: pd.DataFrame = None, y: pd.Series = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def split_train_test(self, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        # Split data into train and test sets\n",
    "        pass\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, X_train: pd.DataFrame = None, y_train: pd.Series = None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def train(self):\n",
    "        # Train a model on the train set\n",
    "        pass\n",
    "\n",
    "\n",
    "class ModelTester:\n",
    "    def __init__(self, model = None, X_test: pd.DataFrame = None, y_test: pd.Series = None):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def test(self) -> float:\n",
    "        # Test the trained model on the test set and return accuracy score\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (main, Dec  7 2022, 08:49:13) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
